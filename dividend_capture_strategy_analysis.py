# -*- coding: utf-8 -*-
"""Dividend Capture Strategy Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/132qsZeiKJKRb5mHMOW-mEl47vtdAcbEY

<h1> Dividend Capture Strategy Analysis </h1>
"""

import pandas as pd
import numpy as np
import os
import datetime
os.getcwd()

"""Let's read in our dataset and look at its structure. We have company identifiers, dates, and then information about the company and the dividend event. Note that we don't have consecutive dates; we have only isolated dates that correspond to an "event" -- the payment of a dividend."""

div = pd.read_csv('CRSP_Dividends.csv',parse_dates=[1])
div

"""__Step 1: Clean and describe the data__

Here I am going to remove all cases where there is zero volume traded. We are trying to see the daily price response to the payment of a dividend, and if there are no trades then we can't expect to get a reliable indicator of the true price, or what price we might have actually been able to trade at. In general it's best to remove circumstances like this, with no liquidity, from your analysis.

I'm also calculating the prior closing price using the RETX (price return without dividend). It's important to use that rather than the RET, which includes the dividend as part of the return and would therefore deliver a faulty prior closing price.

Otherwise, calculating market capitalization and some cleanup as described by the question.
"""

div=div[div['VOL']>0]
div=div[div['DIVAMT']>0.01]
div['PRC']=abs(div['PRC'])
div['PriorClose'] = div['PRC']/(1+div['RETX'])
div['mkcap']=div['SHROUT']*div['PRC']
div=div[div['mkcap'] > 50000]

"""__Step 2. Examine price change relative to dividend__

We can now look at the ratio between the price change and the dividend amount. Our prediction, consistent with the literature, is that the price change should be less than the dividend amount, and this ratio should be less than 1.

Also calculating the stock yield (based upon this single dividend payment), the spread, and the year of the event.
"""

div['ratio'] = (div['PriorClose']-div['PRC'])/div['DIVAMT']
div['yield']=div['DIVAMT']/div['PriorClose']
div['spread']=2*(div['ASK']-div['BID'])/(div['ASK']+div['BID'])
div['year']=pd.DatetimeIndex(div['date']).year

stats=div.describe()
stats

stats['ratio']

"""So the average price drop is only 64% of the amount of the dividend payment -- matches up well to what has been documented in prior work. We see extreme values at the min and the max; may want to windsorize or otherwise handle extreme events like this.

This indiates that there could indeed be a profitable "dividend caputure" strategy. This is an event where the price response across a large number of observations does not match up to that predicted by the efficient markets hypothesis. We could potentially make money by buying at the close, receiving the dividend, and then selling our shares -- likely we are selling our shares at a loss, but the loss will be less than the amount of the dividend recieved.

__Step 2 Part 2: Describe returns to buying close-to-close__

First we need to add the risk-free rate; we can mostly copy the work from HW#2
"""

irx = pd.read_csv('^IRX.csv',  parse_dates=[0])
irx.head()

irx = pd.read_csv('^IRX.csv',  parse_dates=[0])
irx['Tbill yield'] = irx['Close']
irx['Tbill yield filled'] = irx['Tbill yield'].fillna(method='ffill')
irx['dow'] = irx['Date'].dt.dayofweek
irx['lagdow'] = irx['dow'].shift()
irx['numdays'] = 1
irx.loc[irx['dow'] > (irx['lagdow']+1), 'numdays'] = irx['dow'] - irx['lagdow']
irx.loc[irx['dow'] < irx['lagdow'], 'numdays'] = 3 + irx['dow'] + (4 - irx['lagdow'])
irx.loc['2001-09-17','numdays'] = 7
irx['Tbill ret'] = irx['Tbill yield filled'].shift() * irx['numdays'] / 365 / 100

irx = irx[['Date','Tbill ret']]
irx.rename(columns={"Date": "date"},inplace=True)

div.sort_values(by='date',inplace=True)

"""Our events aren't sorted by date; nor are they consecutive days. So we can't just port over the column from one data set to the other as we did in homework 2. Here a merge is more appropriate."""

div = div.merge(irx, how='left', on=['date'])

div['ex ret'] = div['RET'] - div['Tbill ret']
stats=div.describe()

print('Annualized return: ',stats.loc['mean','RET']*252)
print('Annualized standard deviation: ', stats.loc['std','RET']*252**0.5)

t=stats.loc['mean','RET']*stats.loc['count','RET']**0.5/stats.loc['std','RET']
sharpe = stats.loc['mean','ex ret']/stats.loc['std','RET']

t

sharpe*252**0.5

"""The results here are promising. If we were to just buy stocks at the close prior to their ex-div day, and sell at the subsequent close, we earn the RET (which is the price change plus the dividend). That annualized return is quite large at 60.9%. The standard deviation of those returns is also large, annualized at 35.8%. (Note that this is across events, not across days.)

The t-stat on whether this event return is different from zero is 60.6 (!!) and a "Sharpe" ratio across events comes in at 1.61 -- quite promising!

__Step 3: Add betas and correct for CAPM__

I'll read in the betas, construct a year variable for merging (making sure to match the beta from the prior year to all the events for the subsequent year), and sort by PERMNO and YEAR for the merge. I'll also sort my dividend data.
"""

betas = pd.read_csv('Yearly Betas.csv',parse_dates=[1])

betas=betas[['PERMNO','DATE','b_mkt']]
betas['YEAR'] = pd.DatetimeIndex(betas['DATE']).year
betas['YEAR'] = betas['YEAR'] +1
betas.drop(['DATE'],axis=1,inplace=True)
betas.sort_values(by=['PERMNO','YEAR'],inplace=True)

div['YEAR']=pd.DatetimeIndex(div['date']).year
div.sort_values(by=['PERMNO','YEAR'],inplace=True)

betas.head()

div = div.merge(betas, how='left', on=['PERMNO','YEAR'])

"""Here I'm going to create a new data set that removes those events where we couldn't match a beta. I don't necessarily want to drop those permanently though."""

div_betas = div.dropna(subset=['b_mkt'])

div_betas['capm_ret'] = div_betas['b_mkt']*(div_betas['vwretd'] - div_betas['Tbill ret'])
div_betas['ab_ret'] = div_betas['ex ret'] - div_betas['capm_ret']
stats = div_betas.describe()

print('Annualized return: ',stats.loc['mean','ab_ret']*252)
print('Annualized standard deviation: ', stats.loc['std','ab_ret']*252**0.5)
print('Average beta: ', stats.loc['mean','b_mkt'])

t=stats.loc['mean','ab_ret']*stats.loc['count','ab_ret']**0.5/stats.loc['std','ab_ret']
t

sharpe = stats.loc['mean','ab_ret']/stats.loc['std','RET']
sharpe*252**0.5

"""Results are as we might expect. Returns are lower than before, because some of the return we were capturing wasn't due to the dividend capture event but rather the upward drift of the market over the course of the day, as noted in the literature. We have successfully adjusted for this by removing the CAPM expected return and examining the residual (abnormal return).

Those returns are still 48% per year, much higher than zero, and significantly so with a t-stat of 50. The "Sharpe" ratio is also lower than before, as we would expect.

__Step 4: compute a strategy of buying at prior close and selling at the open__

This is relatively simple; we buy at the prior close, and receive the open price plus the dividend payment. We will have to calculate this return ourselves however; niether the RET nor RETX is the right number here as those represent close-to-close returns.

Note that here I have gone back to using the full sample, without removing the events that didn't have a beta matched.
"""

div['CO_ret'] = (div['OPENPRC'] + div['DIVAMT']) / div['PriorClose'] -1
stats = div.describe()

print('Annualized return: ',stats.loc['mean','CO_ret']*252)
print('Annualized standard deviation: ', stats.loc['std','CO_ret']*252**0.5)

t=stats.loc['mean','CO_ret']*stats.loc['count','CO_ret']**0.5/stats.loc['std','CO_ret']
sharpe = stats.loc['mean','CO_ret']/stats.loc['std','CO_ret']

t

sharpe*252**0.5

"""Results relatively consistent here; returns a bit lower, and volatility also lower than our CAPM-corrected residuals.

__Step 5: account for transaction costs__

The Kalay paper proposes that transaction costs are responsible for the lower price drop than predicted by efficient markets. That is, arbitrageurs are not capturing this spread because it is too costly to trade these stocks and the transaction costs will eat up all of the expected return. We can test that by simply subracting off the transaction costs from the full-day returns.

Note that here we are using the spread from the close at the end of the event day. To be precise, we should use the bid/ask spread from both the day prior and the event day (and average the two). This is okay as long as the spread as a percent is stable over time (and evidence supports that).
"""

div['tcost']=div['RET']-div['spread']

div['tcost'].describe()

"""This supports the Kalay observation. Once we account for the bid/ask spread, our returns are actually negative! And actually pretty close to zero -- it comes to about -2% annualized. So this makes sense in the end -- prices don't adjust fully, but what looks like a compelling opportunity on paper won't work in reality once we incorporate the necessary transaction costs.

__Step 6: rank by a metric__

Here we want to use a ranking procedure and see if we can sort the data in a way that might lead to better opportunities. I wrote a function that will take the metric of interest and then do the ranking and output some useful information. This will make it easy to look at a variety of metrics.

The function will output the results grouped by decile for the ranking metric itself; the returns for that decile; the average spread for that decile; and the after-transaction costs return for that decile. We are interested to see if we can find a spread for the last item here.
"""

def deciles(metric):
    div['QUINTILE'] = pd.qcut(div[metric],q=10, labels=range(1,11))
    print('Metric: ', div.groupby(['QUINTILE'])[metric].mean() )
    print()
    print('Returns: ', div.groupby(['QUINTILE'])['RET'].mean() )
    print()
    print('Spreads: ', div.groupby(['QUINTILE'])['spread'].mean())
    print()
    print('After costs: ', div.groupby(['QUINTILE'])['tcost'].mean() )

"""One idea might be market cap -- small stocks might be a less efficient space and allow for greater returns to persist. Let's check!"""

deciles('mkcap')

"""Interesting -- small cap stocks (starting in decile 1) show returns to this event three times as large as the largest cap stocks! However, it is also more costly to trade small cap stocks. The average spread is about 15 times as large! So when we look at the after cost returns, the smallest decile actually shows the worst returns, and the largest shows the best. Returns are negative across all deciles, however."""

deciles('b_mkt')

"""Not too much going on with beta; the highest beta stocks are the cheapest to trade, and therefore have the best after-cost returns, but still negative."""

deciles('yield')

"""The Kalay paper noted some differences in the price change/dividend ratio across yields, and we see that as well. However the higher yielding stocks also have higher spreads. So, everything still negative.

It seems that transaction costs are the real culprit; higher returns (on paper) seem to come along with higher transaction costs. What if we actually sorted on the transaction cost variable itself? Note that I am cheating a little bit here; the spread is forward-looking information and not known prior to the event day. I'm relying on the spread being stable through time (if I sorted on yesterday's spread, I would hope to get very similar results.) But this is an assumption and we would definitely want to check that before proceeding much further.
"""

deciles('spread')

"""So this is interesting. Spreads rise across deciles (by construction; we've sorted on this). Returns also rise across deciles; The stocks that are most costly to trade also have the highest returns. (This is not quite the same thing as saying that the stocks that have the highest returns also have the highest cost to trade.) As it turns out, the spreads rise much faster than the returns. So perhaps we have a possibility: by focusing on the stocks that are the most liquid and have the lowest spreads, we may uncover a profiatable opportunity.

Let's pull out that first decile with the lowest spreads and examine it in more detail.
"""

div_liquid = div[div['QUINTILE'] == 1]
div_liquid['ex ret tcost'] = div_liquid['tcost'] - div_liquid['Tbill ret']
stats = div_liquid.describe()

print('Annualized return: ',stats.loc['mean','tcost']*252)
print('Annualized standard deviation: ', stats.loc['std','tcost']*252**0.5)

t=stats.loc['mean','tcost']*stats.loc['count','tcost']**0.5/stats.loc['std','tcost']
sharpe = stats.loc['mean','ex ret tcost']/stats.loc['std','ex ret tcost']*252**0.5

t

sharpe

div_liquid['mkcap'].mean()

"""Results have definitely deteriorated, but are still positive (32% annualized) with a strong t-stat and compelling Sharpe ratio.

What would next steps be? First verify with prior-day spreads to make this an implementable strategy. Then construct a calendar time return series where you average your exposure across all the available dividend payers on a given day. (How do you handle days for which you don't have a dividend event?) Then control for risk, look for out-of-sample evidence, see if spreads/returns have changed over time.
"""